{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.tokenize import NLTKWordTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tknzr = NLTKWordTokenizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('Predictive_Data/train_file.csv')\n",
    "test_data = pd.read_csv('Predictive_Data/test_file.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>UsageClass</th>\n",
       "      <th>CheckoutType</th>\n",
       "      <th>CheckoutYear</th>\n",
       "      <th>CheckoutMonth</th>\n",
       "      <th>Checkouts</th>\n",
       "      <th>Title</th>\n",
       "      <th>Creator</th>\n",
       "      <th>Subjects</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>PublicationYear</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Tidal wave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tsunamis, Tsunamis Juvenile literature</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>London holiday / Richard Peck.</td>\n",
       "      <td>Peck, Richard, 1934-</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Viking,</td>\n",
       "      <td>1998.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>Cinco de Mayo : celebrating Hispanic pride / C...</td>\n",
       "      <td>Gnojewski, Carol</td>\n",
       "      <td>Cinco de Mayo Mexican holiday History Juvenile...</td>\n",
       "      <td>Enslow Publishers,</td>\n",
       "      <td>c2002.</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>War stories, Historical fiction, Domestic fict...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Physical</td>\n",
       "      <td>Horizon</td>\n",
       "      <td>2005</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>As a man thinketh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thought and thinking</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID UsageClass CheckoutType  CheckoutYear  CheckoutMonth  Checkouts  \\\n",
       "0   1   Physical      Horizon          2005              4          1   \n",
       "1   2   Physical      Horizon          2005              4          1   \n",
       "2   3   Physical      Horizon          2005              4          3   \n",
       "3   4   Physical      Horizon          2005              4          1   \n",
       "4   5   Physical      Horizon          2005              4          1   \n",
       "\n",
       "                                               Title               Creator  \\\n",
       "0                                         Tidal wave                   NaN   \n",
       "1                     London holiday / Richard Peck.  Peck, Richard, 1934-   \n",
       "2  Cinco de Mayo : celebrating Hispanic pride / C...      Gnojewski, Carol   \n",
       "3                                          Annapolis                   NaN   \n",
       "4                                  As a man thinketh                   NaN   \n",
       "\n",
       "                                            Subjects           Publisher  \\\n",
       "0             Tsunamis, Tsunamis Juvenile literature                 NaN   \n",
       "1                                                NaN             Viking,   \n",
       "2  Cinco de Mayo Mexican holiday History Juvenile...  Enslow Publishers,   \n",
       "3  War stories, Historical fiction, Domestic fict...                 NaN   \n",
       "4                               Thought and thinking                 NaN   \n",
       "\n",
       "  PublicationYear MaterialType  \n",
       "0             NaN         BOOK  \n",
       "1           1998.         BOOK  \n",
       "2          c2002.         BOOK  \n",
       "3             NaN         BOOK  \n",
       "4             NaN         BOOK  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'] = train_data['Title'] + train_data['Subjects']\n",
    "test_data['text'] = test_data['Title'] + test_data['Subjects']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[['ID','text','MaterialType']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[['ID','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOOK         21707\n",
       "SOUNDDISC     4149\n",
       "VIDEOCASS     2751\n",
       "VIDEODISC     1420\n",
       "SOUNDCASS     1020\n",
       "MIXED          347\n",
       "MUSIC          165\n",
       "CR              94\n",
       "Name: MaterialType, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.MaterialType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Tidal waveTsunamis, Tsunamis Juvenile literature\n",
       "1                                                      NaN\n",
       "2        Cinco de Mayo : celebrating Hispanic pride / C...\n",
       "3        AnnapolisWar stories, Historical fiction, Dome...\n",
       "4                    As a man thinkethThought and thinking\n",
       "                               ...                        \n",
       "31648    California campingCalifornia Guidebooks, Camp ...\n",
       "31649    silent world of Nicholas QuinnMorse Inspector ...\n",
       "31650    big LebowskiVideo recordings for the hearing i...\n",
       "31651    Fables. [3], Storybook love / Bill Willingham,...\n",
       "31652                                                  NaN\n",
       "Name: text, Length: 31653, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.fillna(\"Not Available\")\n",
    "test_data=test_data.fillna(\"Not Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df,column_name):\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.lower())\n",
    "    df[column_name] = df[column_name].str.replace(\"[^a-zA-Z0-9]\", \" \")\n",
    "    df[column_name] = df[column_name].apply(lambda x: nltk_tknzr.tokenize(x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    df[column_name] = df[column_name].apply(lambda x:' '.join(x))\n",
    "    df[column_name] = df[column_name].apply(lambda x: x.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = clean_text(train_data,'text')\n",
    "test_df = clean_text(test_data,'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>tidal wavetsunamis tsunamis juvenile literature</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>available</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cinco de mayo celebrating hispanic pride carol...</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>annapoliswar stories historical fiction domest...</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>man thinkeththought thinking</td>\n",
       "      <td>BOOK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               text MaterialType\n",
       "0   1    tidal wavetsunamis tsunamis juvenile literature         BOOK\n",
       "1   2                                          available         BOOK\n",
       "2   3  cinco de mayo celebrating hispanic pride carol...         BOOK\n",
       "3   4  annapoliswar stories historical fiction domest...         BOOK\n",
       "4   5                       man thinkeththought thinking         BOOK"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21707\n",
       "2     4149\n",
       "3     2751\n",
       "8     1420\n",
       "4     1020\n",
       "5      347\n",
       "6      165\n",
       "7       94\n",
       "Name: MaterialType, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.MaterialType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'BOOK':1,'SOUNDDISC':2,'VIDEOCASS':3,'VIDEODISC':8,'SOUNDCASS':4, 'MIXED':5, 'MUSIC':6, 'CR':7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.MaterialType = train_df.MaterialType.apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from torchtext import data , vocab\n",
    "import torch\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "from torchtext.datasets import text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize=\"spacy\",\n",
    "                  sequential=True,\n",
    "                  batch_first=True,\n",
    "                  include_lengths=True,\n",
    "                  lower=True,\n",
    "                  stop_words=set(stopwords.words('english')))\n",
    "LABEL = data.LabelField(batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [(None, None),('text',TEXT),('MaterialType', LABEL)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/example.py:68: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n",
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['tidal', 'wavetsunamis', 'tsunamis', 'juvenile', 'literature'], 'MaterialType': 'BOOK'}\n"
     ]
    }
   ],
   "source": [
    "training_data = data.TabularDataset(path='train.csv',\n",
    "                                    format='csv',\n",
    "                                    fields=fields,\n",
    "                                    skip_header=True)\n",
    "\n",
    "# print preprocessed text\n",
    "print(vars(training_data.examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "train_data, valid_data = training_data.split(split_ratio=0.9,stratified=True,strata_field='MaterialType',random_state = random.seed(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocabulary from glove cache\n",
    "vec = torchtext.vocab.GloVe(name='6B', dim=300,cache='/Users/subir/Downloads/glove/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize glove embeddings\n",
    "TEXT.build_vocab(train_data, min_freq=3, vectors=vec,unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 11771\n",
      "Size of LABEL vocabulary: 8\n",
      "[('fiction', 25785), ('juvenile', 9143), ('music', 5175), ('literature', 5036), ('states', 4451), ('united', 4232), ('films', 3964), ('history', 3334), ('drama', 2453), ('recordings', 2132)]\n"
     ]
    }
   ],
   "source": [
    "# No. of unique tokens in text\n",
    "print(\"Size of TEXT vocabulary:\", len(TEXT.vocab))\n",
    "\n",
    "# No. of unique tokens in label\n",
    "print(\"Size of LABEL vocabulary:\", len(LABEL.vocab))\n",
    "\n",
    "# Commonly used words\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# check whether cuda is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Load an iterator\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.text),\n",
    "    sort_within_batch=True,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim,\n",
    "                 n_layers, bidirectional, dropout,pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.RNN(embedding_dim,\n",
    "                            hidden_dim,\n",
    "                            num_layers=n_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            dropout=dropout,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, text, text_lengths):\n",
    "        # text = [batch size,sent_length]\n",
    "        embedded = self.embedding(text)\n",
    "        # embedded = [batch size, sent_len, emb dim]\n",
    "        # packed sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded,\n",
    "                                                            text_lengths,\n",
    "                                                            batch_first=True)\n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        # hidden = [batch size, num layers * num directions,hid dim]\n",
    "        # cell = [batch size, num layers * num directions,hid dim]\n",
    "        # concat the final forward and backward hidden state\n",
    "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        # hidden = [batch size, hid dim * num directions]\n",
    "        dense_outputs = self.fc(hidden)\n",
    "        # Final activation function\n",
    "        outputs = self.act(dense_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define hyperparameters\n",
    "size_of_vocab = len(TEXT.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = len(LABEL.vocab)\n",
    "num_layers = 2\n",
    "bidirection = True\n",
    "dropout = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "# instantiate the model\n",
    "model = classifier(size_of_vocab,\n",
    "                   embedding_dim,\n",
    "                   num_hidden_nodes,\n",
    "                   num_output_nodes,\n",
    "                   num_layers,\n",
    "                   bidirectional=True,\n",
    "                   dropout=dropout,\n",
    "                  pad_idx=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier(\n",
      "  (embedding): Embedding(11771, 300, padding_idx=1)\n",
      "  (lstm): LSTM(300, 32, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=8, bias=True)\n",
      "  (act): Sigmoid()\n",
      ")\n",
      "The model has 3,642,412 trainable parameters\n",
      "torch.Size([11771, 300])\n"
     ]
    }
   ],
   "source": [
    "#architecture\n",
    "print(model)\n",
    "\n",
    "#No. of trianable parameters\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "#Initialize the pretrained embedding\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "\n",
    "# define metric\n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
    "    correct = max_preds.squeeze(1).eq(y)\n",
    "    return correct.sum() / torch.FloatTensor([y.shape[0]])\n",
    "\n",
    "\n",
    "# push to cuda if available\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler):\n",
    "\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # set the model in training phase\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        # resets the gradients after every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # retrieve text and no. of words\n",
    "        text, text_lengths = batch.text\n",
    "\n",
    "        # convert to 1D tensor\n",
    "        predictions = model(text, text_lengths).squeeze()\n",
    "\n",
    "        # compute the loss\n",
    "        loss = criterion(predictions, batch.MaterialType)\n",
    "\n",
    "        # compute the binary accuracy\n",
    "        acc = categorical_accuracy(predictions, batch.MaterialType)\n",
    "\n",
    "        # backpropage the loss and compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        #loss and accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    # initialize every epoch\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    # deactivating dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    # deactivates autograd\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            # retrieve text and no. of words\n",
    "            text, text_lengths = batch.text\n",
    "\n",
    "            # convert to 1d tensor\n",
    "            predictions = model(text, text_lengths).squeeze()\n",
    "\n",
    "            # compute loss and accuracy\n",
    "            loss = criterion(predictions, batch.MaterialType)\n",
    "            acc = categorical_accuracy(predictions, batch.MaterialType)\n",
    "\n",
    "            # keep track of loss and accuracy\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subir/Environment/python/base/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\n",
      "\tTrain Loss: 1.478 | Train Acc: 78.61%\n",
      "\t Val. Loss: 1.402 |  Val. Acc: 85.23%\n",
      "Epoch: 1\n",
      "\n",
      "\tTrain Loss: 1.391 | Train Acc: 86.02%\n",
      "\t Val. Loss: 1.399 |  Val. Acc: 85.14%\n",
      "Epoch: 2\n",
      "\n",
      "\tTrain Loss: 1.385 | Train Acc: 86.45%\n",
      "\t Val. Loss: 1.397 |  Val. Acc: 85.55%\n",
      "Epoch: 3\n",
      "\n",
      "\tTrain Loss: 1.382 | Train Acc: 86.71%\n",
      "\t Val. Loss: 1.394 |  Val. Acc: 85.64%\n",
      "Epoch: 4\n",
      "\n",
      "\tTrain Loss: 1.380 | Train Acc: 87.00%\n",
      "\t Val. Loss: 1.394 |  Val. Acc: 85.71%\n",
      "Epoch: 5\n",
      "\n",
      "\tTrain Loss: 1.378 | Train Acc: 87.21%\n",
      "\t Val. Loss: 1.393 |  Val. Acc: 85.68%\n",
      "Epoch: 6\n",
      "\n",
      "\tTrain Loss: 1.375 | Train Acc: 87.42%\n",
      "\t Val. Loss: 1.394 |  Val. Acc: 85.45%\n",
      "Epoch: 7\n",
      "\n",
      "\tTrain Loss: 1.373 | Train Acc: 87.53%\n",
      "\t Val. Loss: 1.394 |  Val. Acc: 85.74%\n",
      "Epoch: 8\n",
      "\n",
      "\tTrain Loss: 1.371 | Train Acc: 87.56%\n",
      "\t Val. Loss: 1.395 |  Val. Acc: 85.61%\n",
      "Epoch: 9\n",
      "\n",
      "\tTrain Loss: 1.369 | Train Acc: 87.81%\n",
      "\t Val. Loss: 1.398 |  Val. Acc: 85.68%\n",
      "Epoch: 10\n",
      "\n",
      "\tTrain Loss: 1.368 | Train Acc: 87.89%\n",
      "\t Val. Loss: 1.397 |  Val. Acc: 85.52%\n",
      "Epoch: 11\n",
      "\n",
      "\tTrain Loss: 1.367 | Train Acc: 87.97%\n",
      "\t Val. Loss: 1.399 |  Val. Acc: 85.64%\n",
      "Epoch: 12\n",
      "\n",
      "\tTrain Loss: 1.366 | Train Acc: 88.05%\n",
      "\t Val. Loss: 1.398 |  Val. Acc: 85.49%\n",
      "Epoch: 13\n",
      "\n",
      "\tTrain Loss: 1.365 | Train Acc: 88.06%\n",
      "\t Val. Loss: 1.398 |  Val. Acc: 85.49%\n",
      "Epoch: 14\n",
      "\n",
      "\tTrain Loss: 1.364 | Train Acc: 88.05%\n",
      "\t Val. Loss: 1.401 |  Val. Acc: 85.23%\n",
      "Epoch: 15\n",
      "\n",
      "\tTrain Loss: 1.363 | Train Acc: 88.14%\n",
      "\t Val. Loss: 1.400 |  Val. Acc: 85.49%\n",
      "Epoch: 16\n",
      "\n",
      "\tTrain Loss: 1.363 | Train Acc: 88.17%\n",
      "\t Val. Loss: 1.400 |  Val. Acc: 85.49%\n",
      "Epoch: 17\n",
      "\n",
      "\tTrain Loss: 1.362 | Train Acc: 88.17%\n",
      "\t Val. Loss: 1.400 |  Val. Acc: 85.52%\n",
      "Epoch: 18\n",
      "\n",
      "\tTrain Loss: 1.362 | Train Acc: 88.19%\n",
      "\t Val. Loss: 1.401 |  Val. Acc: 85.49%\n",
      "Epoch: 19\n",
      "\n",
      "\tTrain Loss: 1.362 | Train Acc: 88.20%\n",
      "\t Val. Loss: 1.399 |  Val. Acc: 85.58%\n",
      "Epoch: 20\n",
      "\n",
      "\tTrain Loss: 1.361 | Train Acc: 88.22%\n",
      "\t Val. Loss: 1.401 |  Val. Acc: 85.39%\n",
      "Epoch: 21\n",
      "\n",
      "\tTrain Loss: 1.361 | Train Acc: 88.20%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 85.01%\n",
      "Epoch: 22\n",
      "\n",
      "\tTrain Loss: 1.361 | Train Acc: 88.26%\n",
      "\t Val. Loss: 1.402 |  Val. Acc: 85.39%\n",
      "Epoch: 23\n",
      "\n",
      "\tTrain Loss: 1.361 | Train Acc: 88.25%\n",
      "\t Val. Loss: 1.403 |  Val. Acc: 85.27%\n",
      "Epoch: 24\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.27%\n",
      "\t Val. Loss: 1.403 |  Val. Acc: 85.17%\n",
      "Epoch: 25\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.27%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 85.04%\n",
      "Epoch: 26\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.30%\n",
      "\t Val. Loss: 1.403 |  Val. Acc: 85.14%\n",
      "Epoch: 27\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.31%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 28\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.28%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 84.95%\n",
      "Epoch: 29\n",
      "\n",
      "\tTrain Loss: 1.360 | Train Acc: 88.31%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 85.08%\n",
      "Epoch: 30\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.33%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 85.04%\n",
      "Epoch: 31\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.34%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 84.98%\n",
      "Epoch: 32\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.32%\n",
      "\t Val. Loss: 1.404 |  Val. Acc: 85.08%\n",
      "Epoch: 33\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.32%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 84.98%\n",
      "Epoch: 34\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.35%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 85.04%\n",
      "Epoch: 35\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.32%\n",
      "\t Val. Loss: 1.405 |  Val. Acc: 85.04%\n",
      "Epoch: 36\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.35%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.08%\n",
      "Epoch: 37\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.35%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.08%\n",
      "Epoch: 38\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.34%\n",
      "\t Val. Loss: 1.407 |  Val. Acc: 84.95%\n",
      "Epoch: 39\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.35%\n",
      "\t Val. Loss: 1.407 |  Val. Acc: 84.95%\n",
      "Epoch: 40\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.36%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 41\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.36%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 42\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.34%\n",
      "\t Val. Loss: 1.407 |  Val. Acc: 84.89%\n",
      "Epoch: 43\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.38%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 44\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.34%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 45\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.36%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 46\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.35%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 84.98%\n",
      "Epoch: 47\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.37%\n",
      "\t Val. Loss: 1.406 |  Val. Acc: 85.01%\n",
      "Epoch: 48\n",
      "\n",
      "\tTrain Loss: 1.359 | Train Acc: 88.37%\n",
      "\t Val. Loss: 1.407 |  Val. Acc: 84.98%\n",
      "Epoch: 49\n",
      "\n",
      "\tTrain Loss: 1.358 | Train Acc: 88.40%\n",
      "\t Val. Loss: 1.407 |  Val. Acc: 84.98%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 50\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    # train the model\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion,\n",
    "                                  scheduler)\n",
    "\n",
    "    # evaluate the model\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    # save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    print('Epoch: {}\\n'.format(epoch))\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.393 | Test Acc: 85.68%\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('saved_weights.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# inference\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "def predict_class(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]  # compute no. of words\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "  \n",
    "    tensor = tensor.unsqueeze(1).T  # reshape in form of batch,no. of words\n",
    "    length_tensor = torch.LongTensor(length) \n",
    "    preds = model(tensor, length_tensor)\n",
    "    max_preds = preds.argmax(dim = 1)\n",
    "    return max_preds.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21102it [01:31, 231.78it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "ids = []\n",
    "for row in tqdm(test_df.iterrows()):\n",
    "    ids.append(row[1]['ID'])\n",
    "    prediction.append(predict_class(model, row[1]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(data={'ID':ids,'MaterialType':prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>MaterialType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31654</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31655</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31656</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31657</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31658</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  MaterialType\n",
       "0  31654             0\n",
       "1  31655             2\n",
       "2  31656             1\n",
       "3  31657             0\n",
       "4  31658             2"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.MaterialType=submission.MaterialType.apply(lambda x: LABEL.vocab.itos[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BOOK         15958\n",
       "SOUNDDISC     2753\n",
       "VIDEOCASS     2391\n",
       "Name: MaterialType, dtype: int64"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.MaterialType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('sub.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
